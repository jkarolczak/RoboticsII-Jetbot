{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rnFsqbP6wSt7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZHSMTSi3BmsN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "\n",
    "# define your path below\n",
    "# DATASET_PATH = ... # Ania\n",
    "# DATASET_PATH = ... # Witek\n",
    "DATASET_PATH = 'dataset/'  # Jacek\n",
    "\n",
    "# hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_WORKERS = 1\n",
    "TRAIN_SET_SIZE = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TEikNh9bW_s0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VN82Vw_KAdqw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class StorageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path: str) -> None:\n",
    "        self.path = path\n",
    "        self.dirs = [d for d in os.listdir(self.path) if os.path.isdir(os.path.join(self.path, d))]\n",
    "        self.items = self._get_file_target_map()\n",
    "\n",
    "    def _get_file_target_map(self) -> List[Tuple[str, Tuple[float, float]]]:\n",
    "        result = []\n",
    "        for dir in self.dirs:\n",
    "            labels = pd.read_csv(f'{self.path}/{dir}.csv', header=None, index_col=0, names=['speed', 'turn'])\n",
    "            for file_name in os.listdir(os.path.join(self.path, dir)):\n",
    "                file_path = os.path.join(self.path, dir, file_name)\n",
    "                photo_id = int(re.search(r'\\d+', file_name).group())\n",
    "                if photo_id in labels.index:\n",
    "                    target = labels.loc[photo_id]\n",
    "                    result.append((file_path, (target['speed'], target['turn'], photo_id)))\n",
    "        return result\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        path, target = self.items[idx]\n",
    "        img = torch.tensor(np.asarray(Image.open(path)), dtype=torch.float32)\n",
    "        target = torch.tensor(target[:2], dtype=torch.float32)\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(dataset: StorageDataset, train_size: float = 0.8, shuffle: bool = True):\n",
    "    train_keys, test_keys = set(), set()\n",
    "    train_items, test_items = list(), list()\n",
    "    for item in dataset.items:\n",
    "        key = item[1][2]\n",
    "        if key in train_keys:\n",
    "            train_items.append(item)\n",
    "        elif key in test_keys:\n",
    "            test_items.append(item)\n",
    "        else:\n",
    "            if np.random.random() < train_size:\n",
    "                train_keys.add(key)\n",
    "                train_items.append(item)\n",
    "            else:\n",
    "                test_keys.add(key)\n",
    "                test_items.append(item)\n",
    "    train, test = deepcopy(dataset), deepcopy(dataset)\n",
    "    train.items, test.items = train_items, test_items\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = StorageDataset(DATASET_PATH)\n",
    "train_dataset, test_dataset = train_test_split(dataset, train_size=TRAIN_SET_SIZE, shuffle=True)\n",
    "train = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.backend = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "        self.backend.fc = nn.Linear(in_features=512, out_features=2, bias=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.backend(x)\n",
    "        x = torch.clamp(x, min=-1.0, max=1.0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "resnet = ResNet18()\n",
    "resnet.cuda()\n",
    "resnet = torch.jit.script(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_len = len(train_dataset)\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224, device='cuda')\n",
    "os.makedirs('models', exist_ok=True)\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'epoch {epoch}')\n",
    "    train_loss_history, test_loss_history = [], []\n",
    "\n",
    "    resnet.train()\n",
    "    for i, (x_train, y_train) in enumerate(train):\n",
    "        if not i % 5:\n",
    "            print(f'{i}/{int(train_len / BATCH_SIZE)}', end='\\r')\n",
    "        x_train = torch.movedim(x_train, -1, -3)\n",
    "        x_train = x_train.to(torch.device('cuda'))\n",
    "        y_train = y_train.to(torch.device('cuda'))\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = resnet(x_train)\n",
    "        loss = criterion(y_hat, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_history.append(float(loss.detach().cpu()))\n",
    "    print(f'train loss: {sum(train_loss_history) / len(train_loss_history)}')\n",
    "\n",
    "    resnet.eval()\n",
    "    with torch.no_grad():\n",
    "        for x_test, y_test in test:\n",
    "            x_test = torch.movedim(x_test, -1, -3)\n",
    "            x_test = x_test.to(torch.device('cuda'))\n",
    "            y_test = y_test.to(torch.device('cuda'))\n",
    "            y_hat = resnet(x_test)\n",
    "            loss = criterion(y_hat, y_test)\n",
    "            test_loss_history.append(float(loss.detach().cpu()))\n",
    "    test_loss = sum(test_loss_history) / len(test_loss_history)\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        torch.jit.save(resnet, f'models/resnet_{epoch}_{test_loss:.4f}.pt')\n",
    "        torch.onnx.export(resnet, dummy_input, f'models/resnet_{epoch}_{test_loss:.4f}_opset_11.onnx', opset_version=11)\n",
    "        torch.onnx.export(resnet, dummy_input, f'models/resnet_{epoch}_{test_loss:.4f}_opset_13.onnx', opset_version=13)\n",
    "        print(f'test loss: {test_loss} (best, saving model)')\n",
    "    else:\n",
    "        print(f'test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.backend = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "        self.backend.classifier[1] = nn.Linear(in_features=1280, out_features=2, bias=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.backend(x)\n",
    "        x = torch.clamp(x, min=-1.0, max=1.0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/witold/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "mobilenet = MobileNetV2()\n",
    "mobilenet.cuda()\n",
    "mobilenet = torch.jit.script(mobilenet)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(mobilenet.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "train loss: 0.10409319823742491\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File models/mobilenet_0_0.0869.pt cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 7>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m test_loss \u001B[38;5;241m<\u001B[39m best_loss:\n\u001B[1;32m     37\u001B[0m     best_loss \u001B[38;5;241m=\u001B[39m test_loss\n\u001B[0;32m---> 38\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmobilenet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodels/mobilenet_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mepoch\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mtest_loss\u001B[49m\u001B[38;5;132;43;01m:\u001B[39;49;00m\u001B[38;5;124;43m.4f\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.pt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m     torch\u001B[38;5;241m.\u001B[39monnx\u001B[38;5;241m.\u001B[39mexport(mobilenet, dummy_input, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels/mobilenet_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_opset_11.onnx\u001B[39m\u001B[38;5;124m'\u001B[39m, opset_version\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m11\u001B[39m)\n\u001B[1;32m     40\u001B[0m     torch\u001B[38;5;241m.\u001B[39monnx\u001B[38;5;241m.\u001B[39mexport(mobilenet, dummy_input, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels/mobilenet_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_opset_13.onnx\u001B[39m\u001B[38;5;124m'\u001B[39m, opset_version\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m13\u001B[39m)\n",
      "File \u001B[0;32m~/Studia/Semestr-6/RoboticsII-Jetbot/venv/lib/python3.8/site-packages/torch/jit/_serialization.py:81\u001B[0m, in \u001B[0;36msave\u001B[0;34m(m, f, _extra_files)\u001B[0m\n\u001B[1;32m     79\u001B[0m     _extra_files \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(f, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(f, pathlib\u001B[38;5;241m.\u001B[39mPath):\n\u001B[0;32m---> 81\u001B[0m     \u001B[43mm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_extra_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_extra_files\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     83\u001B[0m     ret \u001B[38;5;241m=\u001B[39m m\u001B[38;5;241m.\u001B[39msave_to_buffer(_extra_files\u001B[38;5;241m=\u001B[39m_extra_files)\n",
      "File \u001B[0;32m~/Studia/Semestr-6/RoboticsII-Jetbot/venv/lib/python3.8/site-packages/torch/jit/_script.py:693\u001B[0m, in \u001B[0;36mRecursiveScriptModule.save\u001B[0;34m(self, f, **kwargs)\u001B[0m\n\u001B[1;32m    687\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msave\u001B[39m(\u001B[38;5;28mself\u001B[39m, f, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    688\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    689\u001B[0m \u001B[38;5;124;03m    save(f, _extra_files={})\u001B[39;00m\n\u001B[1;32m    690\u001B[0m \n\u001B[1;32m    691\u001B[0m \u001B[38;5;124;03m    See :func:`torch.jit.save <torch.jit.save>` for details.\u001B[39;00m\n\u001B[1;32m    692\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 693\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_c\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: File models/mobilenet_0_0.0869.pt cannot be opened."
     ]
    }
   ],
   "source": [
    "train_len = len(train_dataset)\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224, device='cuda')\n",
    "os.makedirs('models', exist_ok=True)\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'epoch {epoch}')\n",
    "    train_loss_history, test_loss_history = [], []\n",
    "\n",
    "    mobilenet.train()\n",
    "    for i, (x_train, y_train) in enumerate(train):\n",
    "        if not i % 5:\n",
    "            print(f'{i}/{int(train_len / BATCH_SIZE)}', end='\\r')\n",
    "        x_train = torch.movedim(x_train, -1, -3)\n",
    "        x_train = x_train.to(torch.device('cuda'))\n",
    "        y_train = y_train.to(torch.device('cuda'))\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = mobilenet(x_train)\n",
    "        loss = criterion(y_hat, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_history.append(float(loss.detach().cpu()))\n",
    "    print(f'train loss: {sum(train_loss_history) / len(train_loss_history)}')\n",
    "\n",
    "    mobilenet.eval()\n",
    "    with torch.no_grad():\n",
    "        for x_test, y_test in test:\n",
    "            x_test = torch.movedim(x_test, -1, -3)\n",
    "            x_test = x_test.to(torch.device('cuda'))\n",
    "            y_test = y_test.to(torch.device('cuda'))\n",
    "            y_hat = mobilenet(x_test)\n",
    "            loss = criterion(y_hat, y_test)\n",
    "            test_loss_history.append(float(loss.detach().cpu()))\n",
    "    test_loss = sum(test_loss_history) / len(test_loss_history)\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        torch.jit.save(mobilenet, f'models/mobilenet_{epoch}_{test_loss:.4f}.pt')\n",
    "        torch.onnx.export(mobilenet, dummy_input, f'models/mobilenet_{epoch}_{test_loss:.4f}_opset_11.onnx', opset_version=11)\n",
    "        torch.onnx.export(mobilenet, dummy_input, f'models/mobilenet_{epoch}_{test_loss:.4f}_opset_13.onnx', opset_version=13)\n",
    "        print(f'test loss: {test_loss} (best, saving model)')\n",
    "    else:\n",
    "        print(f'test loss: {test_loss}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "jetbot-dl-model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}